{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Species\n",
       "Iris-setosa        50\n",
       "Iris-versicolor    50\n",
       "Iris-virginica     50\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"../../resource/Iris.csv\")\n",
    "dataset[\"Species\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2\n",
    "\n",
    "## Overfitting\n",
    "- Overfitting is when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data.\n",
    "- Overfitting is more likely with non-parametric and non-linear models that have more flexibility when learning a target function.\n",
    "- Overfitting is a common problem in machine learning, causing the model to perform poorly on new data.\n",
    "- The model is learning the training data too well, including the noise in the data.\n",
    "- Overfitting is more likely with non-parametric and non-linear models that have more flexibility when learning a target function.\n",
    "- Overfitting can be prevented by using techniques like cross-validation, regularization, and training with more data.\n",
    " - *Inductive Bias* : The set of assumptions that, together with the training data, deductively justify the predictions of the model.\n",
    " - *Regularization* : Regularization is a technique used to reduce the complexity of the model such as to prevent overfitting. It does this by adding a penalty term to the loss function. This term discourages the weights from becoming too large, thus preventing the model from becoming too complex.\n",
    " - *Cross-Validation* : Cross-validation is a technique used to assess the performance of the model. It is used to assess how the results of the model will generalize to an independent data set. It is also used to determine the optimal hyperparameters of the model.\n",
    " - *Training with more data* : Training with more data is a technique used to prevent overfitting. It is used to expose the model to more examples of the target function, thus allowing it to learn the underlying structure of the data.\n",
    "## Underfiting\n",
    "- Underfitting is when a model is too simple to learn the underlying structure of the data.\n",
    "- Underfitting is more likely with parametric and linear models that have less flexibility when learning a target function.\n",
    "- Underfitting can be prevented by using more complex models, using models with more features, and reducing the constraints on the model.\n",
    " - *More complex models* : Using more complex models is a technique used to prevent underfitting. It is used to allow the model to learn the underlying structure of the data.\n",
    " - *More features* : Using models with more features is a technique used to prevent underfitting. It is used to allow the model to learn the underlying structure of the data.\n",
    " - *Reducing the constraints on the model* : Reducing the constraints on the model is a technique used to prevent underfitting. It is used to allow the model to learn the underlying structure of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization\n",
    "- Standardization is a technique used to transform the features of the model to have a mean of 0 and a standard deviation of 1.\n",
    "- Standardization is used to make the features of the model comparable, thus allowing the model to learn the underlying structure of the data.\n",
    "- We achieve standardization by subtracting the mean of the feature from the feature and then dividing by the standard deviation of the feature.\n",
    "- Standardization techniques include Z-score standardization, Min-Max standardization, and Robust standardization.\n",
    " - *Z-score standardization* : Z-score standardization is a technique used to transform the features of the model to have a mean of 0 and a standard deviation of 1. It is used to make the features of the model comparable, thus allowing the model to learn the underlying structure of the data.\n",
    " - *Min-Max standardization* : Min-Max standardization is a technique used to transform the features of the model to have a minimum of 0 and a maximum of 1. It is used to make the features of the model comparable, thus allowing the model to learn the underlying structure of the data.\n",
    " - *Robust standardization* : Robust standardization is a technique used to transform the features of the model to have a median of 0 and a median absolute deviation of 1. It is used to make the features of the model comparable, thus allowing the model to learn the underlying structure of the data.\n",
    "\n",
    "## Feature Scaling\n",
    "- Feature scaling is a technique used to transform the features of the model to have a similar scale.\n",
    "- Feature scaling is used to make the features of the model comparable, thus allowing the model to learn the underlying structure of the data.\n",
    "- Feature scaling techniques include standardization, normalization, and robust standardization.\n",
    " - *Standardization* : Standardization is a technique used to transform the features of the model to have a mean of 0 and a standard deviation of 1. It is used to make the features of the model comparable, thus allowing the model to learn the underlying structure of the data.\n",
    " - *Normalization* : Normalization is a technique used to transform the features of the model to have a minimum of 0 and a maximum of 1. It is used to make the features of the model comparable, thus allowing the model to learn the underlying structure of the data.\n",
    " - *Robust standardization* : Robust standardization is a technique used to transform the features of the model to have a median of 0 and a median absolute deviation of 1. It is used to make the features of the model comparable, thus allowing the model to learn the underlying structure of the data.\n",
    "\n",
    "## PAC Learning\n",
    "- PAC learning is a framework used to analyze the learning process of a model.\n",
    "- PAC learning stands for Probably Approximately Correct learning.\n",
    "- PAC learning is used to analyze the learning process of a model by determining the number of examples required for the model to learn the target function.\n",
    "- PAC learning is used to determine the generalization error of the model.\n",
    "- PAC learning is used to determine the sample complexity of the model.\n",
    "\n",
    "## Handling unbalanced data\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
